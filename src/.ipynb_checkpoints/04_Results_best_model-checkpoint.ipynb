{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e11f57e-8bff-4ab8-9832-e94036bd3d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81529a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Tabelle nel DB: [('chats',)]\n",
      "‚úÖ chats.db - Tabella 'chats'\n",
      "  type_and_id                            token parent     timestamp\n",
      "0        None           [keyword] thedemocrats   None  1.722583e+09\n",
      "1        None  [keyword] makeamericagreatagain   None  1.722583e+09\n",
      "2        None                   [keyword] MAGA   None  1.722583e+09\n",
      "3        None            [keyword] Nikki Haley   None  1.722583e+09\n",
      "4        None  [keyword] Robert F. Kennedy Jr.   None  1.722583e+09\n",
      "‚úÖ discovery_edges.csv.gz, \n",
      "Il timestamp da l'ultima volta che hanno visitato quel gruppo ma questo significa che non √® davvero indicativo di una timeline \n",
      "\n",
      "          type_and_id              parent     timestamp\n",
      "0  channel_1306559115  channel_1840578235  1.722586e+09\n",
      "1  channel_2036850729  channel_1840578235  1.722586e+09\n",
      "2  channel_1941222046  channel_1840578235  1.722586e+09\n",
      "3  channel_1749991917  channel_1840578235  1.722586e+09\n",
      "4  channel_1581117699  channel_1840578235  1.722586e+09\n",
      "‚úÖ first_nodes.csv.gz\n",
      "          type_and_id                    token                      parent\n",
      "0  channel_2036421633               trump2024e         [keyword] Trump2024\n",
      "1  channel_2178554925  biden_has_left_the_chat             [keyword] Biden\n",
      "2  channel_2095394414             speech_biden      [keyword] Joseph Biden\n",
      "3  channel_2202860593       republicanpartyeth  [keyword] Republican party\n",
      "4  channel_2157448164      republican_partysol  [keyword] Republican party\n"
     ]
    }
   ],
   "source": [
    "chats_path = '../material/chats.db'\n",
    "conn = sqlite3.connect(chats_path)\n",
    "cursor=conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables=cursor.fetchall()\n",
    "\n",
    "# ========================\n",
    "# 1. Leggi chats.db (SQLite)\n",
    "# ========================\n",
    "\n",
    "print(\"üìå Tabelle nel DB:\", tables)\n",
    "\n",
    "try:\n",
    "    df_chats = pd.read_sql_query(\"SELECT * FROM chats\", conn)\n",
    "    print(\"‚úÖ chats.db - Tabella 'chats'\")\n",
    "    print(df_chats.head())\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Errore nel leggere la tabella:\", e)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# ========================\n",
    "# 2. Leggi discovery_edges.csv.gz\n",
    "# ========================\n",
    "try:\n",
    "    df_edges = pd.read_csv('../material/discovery_edges.csv.gz')\n",
    "    print(\"‚úÖ discovery_edges.csv.gz, \\n\" \\\n",
    "    \"Il timestamp da l'ultima volta che hanno visitato quel gruppo ma questo significa che non √® davvero indicativo di una timeline \\n\")\n",
    "    print(df_edges.head())\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Errore nel leggere discovery_edges:\", e)\n",
    "\n",
    "# ========================\n",
    "# 3. Leggi first_nodes.csv.gz\n",
    "# ========================\n",
    "try:\n",
    "    df_first_nodes = pd.read_csv('../material/first_nodes.csv.gz')\n",
    "    print(\"‚úÖ first_nodes.csv.gz\")\n",
    "    print(df_first_nodes.head())\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Errore nel leggere first_nodes:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c54587e-fd42-4a59-a107-a66ec8416234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fare preprocessing dei testi:\n",
    "import os\n",
    "import re\n",
    "from typing import Callable, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import spacy\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "import langdetect\n",
    "\n",
    "\n",
    "class PreProcessing:\n",
    "    \"\"\"Class for performing text preprocessing operations.\n",
    "\n",
    "    Args:\n",
    "        noadverbs (bool, optional): Flag to remove adverbs from the text. Defaults to False.\n",
    "        noadjectives (bool, optional): Flag to remove adjectives from the text. Defaults to False.\n",
    "        noverbs (bool, optional): Flag to remove verbs from the text. Defaults to False.\n",
    "        noentities (bool, optional): Flag to remove named entities from the text. Defaults to False.\n",
    "        language (str, optional): Language for the Spacy model. Defaults to 'en'.\n",
    "        remove_list (bool, optional): Flag to remove a list of words from the text. Defaults to False.\n",
    "\n",
    "    Attributes:\n",
    "        noadverbs (bool): Flag to remove adverbs from the text.\n",
    "        noadjectives (bool): Flag to remove adjectives from the text.\n",
    "        noverbs (bool): Flag to remove verbs from the text.\n",
    "        noentities (bool): Flag to remove named entities from the text.\n",
    "        language (str): Language for the Spacy model.\n",
    "        remove_list (bool): Flag to remove a list of words from the text.\n",
    "        punctuation (str): Regular expression pattern for removing punctuation.\n",
    "        nlp (spacy.Language): Spacy language model.\n",
    "        stopwords (list): List of stopwords.\n",
    "\n",
    "    Methods:\n",
    "        lowercase_unidecode: Converts text to lowercase and removes diacritics.\n",
    "        remove_urls: Removes URLs from the text.\n",
    "        remove_tweet_marking: Removes Twitter mentions and hashtags from the text.\n",
    "        remove_punctuation: Removes punctuation from the text.\n",
    "        remove_repetion: Removes repeated words from the text.\n",
    "        append_stopwords_list: Appends additional stopwords to the existing list.\n",
    "        remove_stopwords: Removes stopwords from the text.\n",
    "        remove_n: Removes words with length less than or equal to n from the text.\n",
    "        remove_numbers: Removes or filters out numbers from the text.\n",
    "        remove_gerund: Removes gerund endings from verbs in the text.\n",
    "        remove_infinitive: Removes infinitive endings from verbs in the text.\n",
    "        filter_by_idf: Filters out words based on their inverse document frequency.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noadverbs: bool = False, noadjectives: bool = False, noverbs: bool = False,\n",
    "                 noentities: bool = False, language: str = 'en', remove_list: bool = False,stopwords=[]):\n",
    "        \"\"\"Initialize the PreProcessing object.\n",
    "\n",
    "        Args:\n",
    "            noadverbs (bool, optional): Flag to indicate whether to remove adverbs. Defaults to False.\n",
    "            noadjectives (bool, optional): Flag to indicate whether to remove adjectives. Defaults to False.\n",
    "            noverbs (bool, optional): Flag to indicate whether to remove verbs. Defaults to False.\n",
    "            noentities (bool, optional): Flag to indicate whether to remove named entities. Defaults to False.\n",
    "            remove_list (bool, optional): Flag to indicate whether to remove stopwords. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.noadverbs = noadverbs\n",
    "        self.noadjectives = noadjectives\n",
    "        self.noverbs = noverbs\n",
    "        self.noentities = noentities\n",
    "        self.remove_list = remove_list\n",
    "        self.punctuation = (\n",
    "                r'\\(|!|\"|#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|\\.|\\/|'\n",
    "                r':|;|<|=|>|\\?|\\@|\\[|\\]|\\^|_|`|\\{|\\}|~|\\||'\n",
    "                r'\\r\\n|\\n|\\r|\\\\\\)'\n",
    "        )\n",
    "        # self.nlp = self._load_spacy_model(language)\n",
    "        # self.stopwords = [unidecode(x).lower() for x in list(self.nlp.Defaults.stop_words)]\n",
    "        self.stopwords=stopwords\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _process_text(self, text: Union[str, list], function: Callable) -> Union[str, list]:\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            return function(text)\n",
    "        elif isinstance(text, list):\n",
    "            return [function(x) for x in text]\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "    def lowercase_unidecode(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Convert the given text to lowercase and remove any diacritical marks (accents).\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text to be processed. It can be either a string or a list of strings.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text. If the input is a string, the output will be a string. If the input is a list,\n",
    "            the output will be a list of strings.\n",
    "\n",
    "        Example:\n",
    "            >>> pre_processor = PreProcessor()\n",
    "            >>> text = \"Caf√©\"\n",
    "            >>> pre_processor.lowercase_unidecode(text)\n",
    "            'cafe'\n",
    "        \"\"\"\n",
    "        from unidecode import unidecode\n",
    "        text = self._process_text(text, lambda value: value.lower())\n",
    "        text = self._process_text(text, unidecode)\n",
    "        return text\n",
    "\n",
    "    def remove_urls(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes URLs from the given text or list of texts.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text or list of texts from which to remove URLs.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The text or list of texts with URLs removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'http\\S+ *', '', value).strip())\n",
    "\n",
    "    def remove_tweet_marking(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes tweet markings (e.g., @mentions and #hashtags) from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text or list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text or list of processed texts with tweet markings removed.\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'(@|#)\\S+ *', '', value).strip())\n",
    "\n",
    "    def remove_html_tags(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes HTML tags from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text or list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text or list of processed texts with HTML tags removed.\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'<.*?> *', '', value).strip())\n",
    "\n",
    "    def remove_punctuation(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes punctuation from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text from which punctuation needs to be removed.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The text with punctuation removed.\n",
    "        \"\"\"\n",
    "        text = self._process_text(text, lambda value: re.sub(self.punctuation, ' ', value))\n",
    "        text = self._process_text(text, lambda value: re.sub(' {2,}', ' ', value).strip())\n",
    "        return text\n",
    "\n",
    "    def remove_repetition(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes repeated words in the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of words.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text with repeated words removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', value))\n",
    "\n",
    "    def append_stopwords_list(self, stopwords: list) -> None:\n",
    "        \"\"\"Appends additional stopwords to the existing list of stopwords.\n",
    "\n",
    "        Parameters:\n",
    "        stopwords (list): A list of stopwords to be appended.\n",
    "\n",
    "        \"\"\"\n",
    "        self.stopwords.extend(stopwords)\n",
    "\n",
    "    def remove_stopwords(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes stopwords from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text from which stopwords need to be removed.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text with stopwords removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(rf'\\b({\"|\".join(self.stopwords)})\\b *', '', value).strip())\n",
    "\n",
    "    \n",
    "\n",
    "    def remove_n(self, text: Union[str, list], n: int) -> Union[str, list]:\n",
    "        \"\"\"Removes words of length 1 to n followed by the word 'pri' from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of texts to process.\n",
    "            n (int): The maximum length of words to remove.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text or list of processed texts.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(rf'(\\b|^)\\w{{1,{n}}}(\\b|$) ?', '', value).strip())\n",
    "\n",
    "    def remove_numbers(self, text: Union[str, list], mode: str = 'replace') -> Union[str, list]:\n",
    "        \"\"\"Removes or replaces numbers in the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of texts.\n",
    "            mode (str, optional): The mode of operation. Defaults to 'replace'.\n",
    "                - 'filter': Removes the numbers from the text.\n",
    "                - 'replace': Replaces the numbers with an empty string.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text or list of processed texts.\n",
    "        \"\"\"\n",
    "        if mode == \"filter\":\n",
    "            return self._process_text(text, lambda value: '' if re.search('[0-9]', value) else value)\n",
    "        elif mode == \"replace\":\n",
    "            return self._process_text(text, lambda value: re.sub('[0-9] *', '', value))\n",
    "\n",
    "    def remove_gerund(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes the gerund form '-ndo' from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text with the gerund form removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'ndo\\b', '', value))\n",
    "\n",
    "    def remove_infinitive(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes the infinitive form of verbs from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text with infinitive forms removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'r\\b', '', value))\n",
    "    \n",
    "    \n",
    "    def detect_language(self,text):\n",
    "        import langdetect\n",
    "        try:\n",
    "            d=langdetect.detect_langs(text)\n",
    "            # Trasforma la lista in un dizionario\n",
    "            langs_dict = {lang.lang: lang.prob for lang in d}\n",
    "            best_lang=max(langs_dict,key=langs_dict.get)\n",
    "            best_lang=best_lang if langs_dict[best_lang]>=0.7 else 'unk'\n",
    "            return best_lang    \n",
    "        except langdetect.LangDetectException as e:\n",
    "            return 'unk'\n",
    "        return None\n",
    "\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "# here the funziona to call to preprocess the text\n",
    "def preprocess_text(text,stopwords=stopwords):\n",
    "    \n",
    "    pp=PreProcessing(language='en',stopwords=stopwords)\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    text = pp.lowercase_unidecode(text)\n",
    "    \n",
    "    if pp.detect_language(text)!='en':\n",
    "        return \"\"\n",
    "    \n",
    "    text = pp.remove_stopwords(text)\n",
    "    text = pp.remove_tweet_marking(text)\n",
    "    text = pp.remove_urls(text)\n",
    "    text = pp.remove_repetition(text)\n",
    "    text = pp.remove_punctuation(text)\n",
    "    text = pp.remove_numbers(text)\n",
    "    text = pp.remove_n(text, n=3)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7006e0-4a03-4a5c-a5a6-6581d35c887e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-------------------------------\n",
      "\n",
      "2-------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 207.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "4-------------------------------\n",
      "\n",
      "        id       user_id                                               text  \\\n",
      "0  89176.0  6.411871e+09  I‚Äôm scouting for new crypto opportunities, any...   \n",
      "\n",
      "      timestamp  bot_flag  via_bot_id  via_business_bot_id  reply_to_msg_id  \\\n",
      "0  1.714622e+09       NaN         NaN                  NaN              NaN   \n",
      "\n",
      "   fwd_flag  fwd_from_id  ... forwards  replies  reactions  reaction_json  \\\n",
      "0       NaN          NaN  ...      NaN      3.0        0.0            NaN   \n",
      "\n",
      "   bot verified first_name last_name username  \\\n",
      "0  NaN      NaN        NaN       NaN      NaN   \n",
      "\n",
      "                       processed_text  \n",
      "0  scouting crypto opportunities tips  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "#df = pd.concat(([pd.read_csv(file, sep='\\t') for file in tqdm(glob('sample/*.csv'))]), ignore_index=True)\n",
    "#df=df.drop_duplicates(subset=['processed_text'])\n",
    "\n",
    "# DA PORTATILE SCOMMENTARE QUESTO\n",
    "#extracted_dir = os.path.join(\"..\", \"material\", \"extracted\")\n",
    "\n",
    "#------------------------------------------------\n",
    "#DA JUPYTER CUSTER SCOMMENTARE QUESTO\n",
    "extracted_dir = os.path.expanduser(\"~/telegram_2024/usc-tg-24-us-election/extracted\")\n",
    "\n",
    "print(\"1-------------------------------\\n\")\n",
    "\n",
    "# cerca tutti i file .tsv.gz dentro tutte le cartelle channel_*\n",
    "all_files = glob(os.path.join(extracted_dir, \"channel_*\", \"*.tsv.gz\"))\n",
    "\n",
    "#take only the first 3 files\n",
    "all_files = all_files[:10]\n",
    "\n",
    "print(\"2-------------------------------\\n\")\n",
    "\n",
    "# leggi tutti i file e concatena in un unico DataFrame\n",
    "df = pd.concat(\n",
    "    [pd.read_csv(file, sep='\\t', compression='gzip') for file in tqdm(all_files)],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(\"3-------------------------------\\n\")\n",
    "\n",
    "# preprocess the text\n",
    "print(preprocess_text(\"Ciao, come va?\") + \"\\n\\n\")\n",
    "\n",
    "df[\"processed_text\"] = df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "print(\"4-------------------------------\\n\")\n",
    "\n",
    "# rimuovi i duplicati sulla colonna \"processed_text\"\n",
    "df = df.drop_duplicates(subset=[\"processed_text\"])\n",
    "\n",
    "print(df.head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 351/352 [13:10<00:02,  2.25s/it]\n",
      "\n",
      "  0%|          | 1/352 [00:00<01:50,  3.16it/s]\u001b[A\n",
      "  1%|          | 4/352 [00:00<00:38,  9.07it/s]\u001b[A\n",
      "  2%|‚ñè         | 7/352 [00:00<00:29, 11.61it/s]\u001b[A\n",
      "  3%|‚ñé         | 9/352 [00:00<00:31, 10.77it/s]\u001b[A\n",
      "  3%|‚ñé         | 11/352 [00:01<00:37,  9.13it/s]\u001b[A\n",
      "  4%|‚ñé         | 13/352 [00:01<00:31, 10.86it/s]\u001b[A\n",
      "  4%|‚ñç         | 15/352 [00:01<00:36,  9.14it/s]\u001b[A\n",
      "  5%|‚ñç         | 17/352 [00:01<00:35,  9.46it/s]\u001b[A\n",
      "  5%|‚ñå         | 19/352 [00:01<00:29, 11.15it/s]\u001b[A\n",
      "  7%|‚ñã         | 24/352 [00:02<00:20, 15.85it/s]\u001b[A\n",
      "  7%|‚ñã         | 26/352 [00:02<00:23, 13.73it/s]\u001b[A\n",
      "  8%|‚ñä         | 28/352 [00:02<00:33,  9.60it/s]\u001b[A\n",
      "  9%|‚ñä         | 30/352 [00:02<00:32,  9.78it/s]\u001b[A\n",
      "  9%|‚ñâ         | 32/352 [00:03<00:41,  7.80it/s]\u001b[A\n",
      "  9%|‚ñâ         | 33/352 [00:03<00:49,  6.41it/s]\u001b[A\n",
      " 10%|‚ñà         | 36/352 [00:04<00:55,  5.73it/s]\u001b[A\n",
      " 11%|‚ñà         | 37/352 [00:04<00:51,  6.14it/s]\u001b[A\n",
      " 11%|‚ñà         | 38/352 [00:04<01:11,  4.40it/s]\u001b[A\n",
      " 11%|‚ñà         | 39/352 [00:05<01:09,  4.52it/s]\u001b[A\n",
      " 11%|‚ñà‚ñè        | 40/352 [00:05<01:00,  5.17it/s]\u001b[A\n",
      " 12%|‚ñà‚ñè        | 41/352 [00:05<01:08,  4.54it/s]\u001b[A\n",
      " 12%|‚ñà‚ñè        | 42/352 [00:05<01:22,  3.76it/s]\u001b[A\n",
      " 12%|‚ñà‚ñè        | 43/352 [00:06<01:24,  3.64it/s]\u001b[A\n",
      " 12%|‚ñà‚ñé        | 44/352 [00:06<01:44,  2.94it/s]\u001b[A\n",
      " 13%|‚ñà‚ñé        | 46/352 [00:06<01:10,  4.32it/s]\u001b[A\n",
      " 14%|‚ñà‚ñé        | 48/352 [00:08<02:23,  2.12it/s]\u001b[A\n",
      " 14%|‚ñà‚ñç        | 49/352 [00:08<02:11,  2.30it/s]\u001b[A\n",
      " 14%|‚ñà‚ñç        | 50/352 [00:09<01:54,  2.64it/s]\u001b[A\n",
      " 14%|‚ñà‚ñç        | 51/352 [00:10<02:41,  1.86it/s]\u001b[A\n",
      " 15%|‚ñà‚ñç        | 52/352 [00:11<03:18,  1.51it/s]\u001b[A\n",
      " 15%|‚ñà‚ñå        | 53/352 [00:11<02:39,  1.87it/s]\u001b[A\n",
      " 16%|‚ñà‚ñå        | 55/352 [00:12<02:40,  1.85it/s]\u001b[A\n",
      " 16%|‚ñà‚ñå        | 56/352 [00:13<03:19,  1.49it/s]\u001b[A\n",
      " 16%|‚ñà‚ñå        | 57/352 [00:13<02:50,  1.73it/s]\u001b[A\n",
      " 16%|‚ñà‚ñã        | 58/352 [00:14<02:36,  1.88it/s]\u001b[A\n",
      " 17%|‚ñà‚ñã        | 59/352 [00:14<02:08,  2.27it/s]\u001b[A\n",
      " 18%|‚ñà‚ñä        | 62/352 [00:15<02:16,  2.12it/s]\u001b[A\n",
      " 18%|‚ñà‚ñä        | 63/352 [00:16<02:24,  2.01it/s]\u001b[A\n",
      " 18%|‚ñà‚ñä        | 64/352 [00:17<02:36,  1.84it/s]\u001b[A\n",
      " 18%|‚ñà‚ñä        | 65/352 [00:17<02:25,  1.97it/s]\u001b[A\n",
      " 19%|‚ñà‚ñâ        | 66/352 [00:17<02:02,  2.33it/s]\u001b[A\n",
      " 19%|‚ñà‚ñâ        | 67/352 [00:17<01:37,  2.93it/s]\u001b[A\n",
      " 19%|‚ñà‚ñâ        | 68/352 [00:19<03:00,  1.58it/s]\u001b[A\n",
      " 20%|‚ñà‚ñâ        | 69/352 [00:19<02:40,  1.76it/s]\u001b[A\n",
      " 20%|‚ñà‚ñâ        | 70/352 [00:19<02:02,  2.30it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà        | 71/352 [00:20<02:48,  1.67it/s]\u001b[A\n",
      " 20%|‚ñà‚ñà        | 72/352 [00:21<02:47,  1.67it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà        | 73/352 [00:21<02:47,  1.66it/s]\u001b[A\n",
      " 21%|‚ñà‚ñà        | 74/352 [00:23<04:50,  1.04s/it]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñè       | 76/352 [00:24<02:55,  1.57it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñè       | 77/352 [00:24<02:51,  1.60it/s]\u001b[A\n",
      " 22%|‚ñà‚ñà‚ñè       | 79/352 [00:25<02:20,  1.95it/s]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñé       | 80/352 [00:29<06:01,  1.33s/it]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñé       | 81/352 [00:30<05:45,  1.27s/it]\u001b[A\n",
      " 23%|‚ñà‚ñà‚ñé       | 82/352 [00:31<04:28,  1.01it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñé       | 83/352 [00:31<03:36,  1.24it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñç       | 84/352 [00:32<03:43,  1.20it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñç       | 85/352 [00:33<03:55,  1.14it/s]\u001b[A\n",
      " 24%|‚ñà‚ñà‚ñç       | 86/352 [00:33<03:02,  1.46it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(cpu_count())\n",
    "\n",
    "# Lista per accumulare tutti i messaggi in inglese\n",
    "all_english_messages = []\n",
    "\n",
    "pp2 = PreProcessing(language='en', noentities=True, stopwords=stopwords)\n",
    "\n",
    "# Wrapper function for multiprocessing\n",
    "def process_file(args):\n",
    "    file, channel_id, token = args\n",
    "    try:\n",
    "        df = pd.read_csv(file, sep='\\t', compression='gzip', usecols=['text'])\n",
    "        df = df.dropna(subset=['text'])\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        \n",
    "        df['text_preprocessed'] = df['text'].apply(preprocess_text)\n",
    "        \n",
    "        df = df[df['text_preprocessed']!=\"\"]\n",
    "        \n",
    "        df['channel_id'] = channel_id\n",
    "        df['token'] = token\n",
    "        return df if not df.empty else None\n",
    "    except Exception as e:\n",
    "        print(f'Errore nel file {file}: {e}')\n",
    "        return None\n",
    "\n",
    "# Prepare list of files to process\n",
    "file_args = []\n",
    "for _, row in df_first_nodes.iterrows():\n",
    "    channel_id = row['type_and_id']\n",
    "    token = row['token']\n",
    "    channel_path = os.path.join(extracted_dir, channel_id)\n",
    "    if not os.path.isdir(channel_path):\n",
    "        continue\n",
    "    files = []\n",
    "    for month in [8, 9, 10, 11]: \n",
    "        month_str = f\"{month:02d}\"  \n",
    "        files.extend(glob(os.path.join(channel_path, f'2024-{month_str}.tsv.gz')))\n",
    "\n",
    "    file_args.extend([(file, channel_id, token) for file in files])\n",
    "\n",
    "# Use multiprocessing to process files\n",
    "results=[]\n",
    "with Pool(cpu_count()) as pool:\n",
    "    pbar=tqdm(total=len(file_args))\n",
    "    for res in pool.imap_unordered(process_file, file_args):\n",
    "        pbar.update(1)\n",
    "        results.append(res)\n",
    "\n",
    "# Filter out None results and concatenate\n",
    "all_english_messages = [df for df in results if df is not None]\n",
    "print(\"--1--\")\n",
    "df_english_preprocessed_messages = pd.concat(all_english_messages, ignore_index=True)\n",
    "print(\"--2--\")\n",
    "df_english_preprocessed_messages=df_english_preprocessed_messages.drop_duplicates(subset=['text_preprocessed'])\n",
    "print(\"--3--\")\n",
    "\n",
    "df_english_preprocessed_messages.to_csv(\"../material/english_preprocessed_messages.tsv.gz\", sep='\\t', index=False, compression='gzip')\n",
    "print(\"--4--\")\n",
    "print(df_english_preprocessed_messages.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd8049-f9ef-4edd-97f4-7ee210cca814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model_name = 'all-distilroberta-v1'\n",
    "embedding_model=SentenceTransformer(model_name)\n",
    "\n",
    "umap_params = {'n_components': 5, 'n_neighbors': 5, 'min_dist': 0.0}\n",
    "\n",
    "hdbscan_params = {'min_cluster_size': 500,'min_samples':100,'prediction_data':True}\n",
    "\n",
    "if os.path.exists(f'final/final_embeddings_{model_name}.npy'):\n",
    "    embeddings = np.load(f'final/final_embeddings_{model_name}.npy')\n",
    "    print(f'embedding {model_name} loaded')\n",
    "else:\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    embeddings = embedding_model.encode(df['processed_text'].tolist(), show_progress_bar=True, device=device)\n",
    "    np.save(f'final/final_embeddings_{model_name}.npy', embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68119eb-a332-4d9a-8183-5cfe72bf9451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af49ede-de11-4ccf-86ba-ef02e259989f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df['processed_text'])\n",
    "# (x,y) x=how many text have been processed, y=how many embeddings have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342381c-933b-46de-9181-fbd3e95d4b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "umap_model = UMAP(**umap_params)\n",
    "hdbscan_model = HDBSCAN(**hdbscan_params)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "max_features_vectorizer = 1024\n",
    "min_df_vectorizer = 0.01\n",
    "max_df_vectorizer = 0.99 \n",
    "\n",
    "#min_df = 0.01 ‚Üí una parola deve apparire in almeno 100 documenti per essere tenuta.\n",
    "#max_df = 0.99 ‚Üí se una parola appare in pi√π di 9.900 documenti, viene scartata (tipo \"the\", \"and\", ecc.).\n",
    "#max_features = 1024 ‚Üí anche se hai 10.000 parole valide, tiene solo le 1024 pi√π frequenti.\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "  max_features=max_features_vectorizer, \n",
    "  min_df=min_df_vectorizer,\n",
    "  max_df=max_df_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b52ac-512b-4e4c-86ca-789cfa3bba1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    verbose=True,\n",
    "    top_n_words=20,\n",
    "    language = 'english', \n",
    "    vectorizer_model=vectorizer_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(df['processed_text'],embeddings=embeddings)\n",
    "print(f\"Execution time for {model_name} UMAP: {time.time()-t0}s\")\n",
    "\n",
    "topics=np.array(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82084cc5-85ea-4562-8908-caba625b95fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.save(f'final/final_{model_name}.pkl')\n",
    "topics=np.array(topic_model.topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a7507-ba78-4825-b2b0-e1eaac0dd449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model=BERTopic.load(f'final/final_{model_name}.pkl')\n",
    "topics=np.array(topic_model.topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08c2cd-c974-430c-9b05-010b5a938544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=-1,n_words=20, width = 350,height=450)\n",
    "\n",
    "#top_n_topics=-1\tMostra tutti i topic (eccetto outlier -1)\ttop_n_topics=5 mostrerebbe solo i primi 5 topic pi√π grandi\n",
    "#n_words=20\tMostra 20 parole per ogni topic nel grafico\tSe vuoi vederne solo 10, metti n_words=10\n",
    "#width=350\tLarghezza (in pixel) del grafico\tCambia la dimensione orizzontale del plot\n",
    "#height=450\tAltezza (in pixel) del grafico\tCambia la dimensione verticale del plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832737dc-03ef-4603-9060-3f9fe1c8e1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = df['processed_text']\n",
    "timestamps = pd.to_datetime(df['date'],format=\"%Y-%m-%d\")\n",
    "\n",
    "topics_over_time = topic_model.topics_over_time(\n",
    "    list(df['processed_text']),\n",
    "    list(df['date']),\n",
    "    nr_bins = timestamps.nunique(),\n",
    "    global_tuning = False,\n",
    "    evolution_tuning = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a204319-726a-4322-a17a-f7a0914cd573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(\n",
    "    topics_over_time,\n",
    "    top_n_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7d7d4-1b49-4e28-aea5-0560db58d86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umap_model=topic_model.umap_model\n",
    "reduced_embeddings=umap_model.transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f9fcb-ee0f-4dfa-9c80-aed2d92ed0a1",
   "metadata": {},
   "source": [
    "- reduce outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3be546-0e58-42a4-906e-bb80a8b0d88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_topics = topic_model.reduce_outliers(list(texts), topics , strategy=\"c-tf-idf\", threshold=0.1)\n",
    "\n",
    "#Quando BERTopic prova a riassegnare un outlier, confronta il suo contenuto testuale con i topic usando una misura di similarit√† tra vettori TF-IDF.\n",
    "#Di solito si tratta di una similarit√† coseno, che varia tra:\n",
    "#0.0 = nessuna somiglianza\n",
    "#1.0 = perfetta somiglianza\n",
    "#0.1 √® molto poco\n",
    "\n",
    "\n",
    "\n",
    "topic_model.update_topics(list(texts), topics=new_topics,\n",
    "                          vectorizer_model=vectorizer_model,top_n_words=20)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1024c9-7f2a-44a6-9517-31d0c220b8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_topics=np.array(new_topics)\n",
    "pd.Series(new_topics).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adcd337-a7ac-4f44-9572-5055aee64b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-bertopic_env]",
   "language": "python",
   "name": "conda-env-.conda-bertopic_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
