{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e11f57e-8bff-4ab8-9832-e94036bd3d6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENBLAS_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhdbscan\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HDBSCAN\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mumap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UMAP\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/bertopic/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbertopic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bertopic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BERTopic\n\u001b[1;32m      5\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertopic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTopic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m ]\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/bertopic/_bertopic.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Models\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhdbscan\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HDBSCAN\n\u001b[1;32m     43\u001b[0m     HAS_HDBSCAN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m):\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/hdbscan/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhdbscan_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HDBSCAN, hdbscan\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobust_single_linkage_\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobustSingleLinkage, robust_single_linkage\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidity\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validity_index\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/hdbscan/hdbscan_.py:9\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mHDBSCAN: Hierarchical Density-Based Spatial Clustering\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m         of Applications with Noise\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator, ClusterMixin\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pairwise_distances\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/sklearn/__init__.py:73\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/sklearn/utils/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/sklearn/utils/_chunking.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/sklearn/utils/validation.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/sklearn/utils/_array_api.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[1;32m     19\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_api_compat.numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/sklearn/utils/fixes.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/scipy/stats/__init__.py:624\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    619\u001b[0m \n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    623\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/scipy/stats/_stats_py.py:8846\u001b[0m\n\u001b[1;32m   8840\u001b[0m     p \u001b[38;5;241m=\u001b[39m _get_pvalue(\u001b[38;5;241m-\u001b[39mwbfn, distribution, alternative, xp\u001b[38;5;241m=\u001b[39mnp)\n\u001b[1;32m   8842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BrunnerMunzelResult(wbfn, p)\n\u001b[1;32m   8845\u001b[0m \u001b[38;5;129;43m@_axis_nan_policy_factory\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSignificanceResult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwd_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaired\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m-> 8846\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mcombine_pvalues\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfisher\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   8847\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m   8848\u001b[0m \u001b[38;5;124;43;03m    Combine p-values from independent tests that bear upon the same hypothesis.\u001b[39;49;00m\n\u001b[1;32m   8849\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8965\u001b[0m \n\u001b[1;32m   8966\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m   8967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marray_namespace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/scipy/stats/_axis_nan_policy.py:665\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator\u001b[0;34m(hypotest_fun_in)\u001b[0m\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tuple_to_result(\u001b[38;5;241m*\u001b[39mres)\n\u001b[1;32m    664\u001b[0m _axis_parameter_doc, _axis_parameter \u001b[38;5;241m=\u001b[39m _get_axis_params(default_axis)\n\u001b[0;32m--> 665\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mFunctionDoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis_nan_policy_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m parameter_names \u001b[38;5;241m=\u001b[39m [param\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parameter_names:\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/scipy/_lib/_docscrape.py:589\u001b[0m, in \u001b[0;36mFunctionDoc.__init__\u001b[0;34m(self, func, role, doc, config)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 589\u001b[0m \u001b[43mNumpyDocString\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/site-packages/scipy/_lib/_docscrape.py:142\u001b[0m, in \u001b[0;36mNumpyDocString.__init__\u001b[0;34m(self, docstring, config)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, docstring, config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    141\u001b[0m     orig_docstring \u001b[38;5;241m=\u001b[39m docstring\n\u001b[0;32m--> 142\u001b[0m     docstring \u001b[38;5;241m=\u001b[39m \u001b[43mtextwrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdedent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocstring\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc \u001b[38;5;241m=\u001b[39m Reader(docstring)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parsed_data \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msections)\n",
      "File \u001b[0;32m~/.conda/envs/bertopic_env/lib/python3.10/textwrap.py:439\u001b[0m, in \u001b[0;36mdedent\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    437\u001b[0m margin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m text \u001b[38;5;241m=\u001b[39m _whitespace_only_re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m--> 439\u001b[0m indents \u001b[38;5;241m=\u001b[39m \u001b[43m_leading_whitespace_re\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indent \u001b[38;5;129;01min\u001b[39;00m indents:\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m margin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c1c43-7dbb-40ec-a7ee-35d7b2e24be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DA PORTATILE SCOMMENTARE QUESTO\n",
    "#extracted_dir = os.path.join(\"..\", \"material\", \"extracted\")\n",
    "\n",
    "#------------------------------------------------\n",
    "#DA JUPYTER CUSTER SCOMMENTARE QUESTO\n",
    "extracted_dir = os.path.expanduser(\"~/telegram_2024/usc-tg-24-us-election/extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81529a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chats_path = '../material/chats.db'\n",
    "conn = sqlite3.connect(chats_path)\n",
    "cursor=conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables=cursor.fetchall()\n",
    "\n",
    "# ========================\n",
    "# 1. Leggi chats.db (SQLite)\n",
    "# ========================\n",
    "\n",
    "print(\"üìå Tabelle nel DB:\", tables)\n",
    "\n",
    "try:\n",
    "    df_chats = pd.read_sql_query(\"SELECT * FROM chats\", conn)\n",
    "    print(\"‚úÖ chats.db - Tabella 'chats'\")\n",
    "    print(df_chats.head())\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Errore nel leggere la tabella:\", e)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# ========================\n",
    "# 2. Leggi discovery_edges.csv.gz\n",
    "# ========================\n",
    "try:\n",
    "    df_edges = pd.read_csv('../material/discovery_edges.csv.gz')\n",
    "    print(\"‚úÖ discovery_edges.csv.gz, \\n\" \\\n",
    "    \"Il timestamp da l'ultima volta che hanno visitato quel gruppo ma questo significa che non √® davvero indicativo di una timeline \\n\")\n",
    "    print(df_edges.head())\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Errore nel leggere discovery_edges:\", e)\n",
    "\n",
    "# ========================\n",
    "# 3. Leggi first_nodes.csv.gz\n",
    "# ========================\n",
    "try:\n",
    "    df_first_nodes = pd.read_csv('../material/first_nodes.csv.gz')\n",
    "    print(\"‚úÖ first_nodes.csv.gz\")\n",
    "    print(df_first_nodes.head())\n",
    "    print(len(df_first_nodes))\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Errore nel leggere first_nodes:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96252cbe-94cd-43b4-90ef-94bf1d01e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"type_and_id unique in df_first_nodes\" + str(df_first_nodes.type_and_id.nunique()))\n",
    "print(\"type_and_id in df_first_nodes\" + str(len(df_first_nodes)))\n",
    "print(\"type_and_id NaN in df_first_nodes \" + str(df_first_nodes['type_and_id'].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c54587e-fd42-4a59-a107-a66ec8416234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fare preprocessing dei testi:\n",
    "import os\n",
    "import re\n",
    "from typing import Callable, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import spacy\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "import langdetect\n",
    "\n",
    "\n",
    "class PreProcessing:\n",
    "    \"\"\"Class for performing text preprocessing operations.\n",
    "\n",
    "    Args:\n",
    "        noadverbs (bool, optional): Flag to remove adverbs from the text. Defaults to False.\n",
    "        noadjectives (bool, optional): Flag to remove adjectives from the text. Defaults to False.\n",
    "        noverbs (bool, optional): Flag to remove verbs from the text. Defaults to False.\n",
    "        noentities (bool, optional): Flag to remove named entities from the text. Defaults to False.\n",
    "        language (str, optional): Language for the Spacy model. Defaults to 'en'.\n",
    "        remove_list (bool, optional): Flag to remove a list of words from the text. Defaults to False.\n",
    "\n",
    "    Attributes:\n",
    "        noadverbs (bool): Flag to remove adverbs from the text.\n",
    "        noadjectives (bool): Flag to remove adjectives from the text.\n",
    "        noverbs (bool): Flag to remove verbs from the text.\n",
    "        noentities (bool): Flag to remove named entities from the text.\n",
    "        language (str): Language for the Spacy model.\n",
    "        remove_list (bool): Flag to remove a list of words from the text.\n",
    "        punctuation (str): Regular expression pattern for removing punctuation.\n",
    "        nlp (spacy.Language): Spacy language model.\n",
    "        stopwords (list): List of stopwords.\n",
    "\n",
    "    Methods:\n",
    "        lowercase_unidecode: Converts text to lowercase and removes diacritics.\n",
    "        remove_urls: Removes URLs from the text.\n",
    "        remove_tweet_marking: Removes Twitter mentions and hashtags from the text.\n",
    "        remove_punctuation: Removes punctuation from the text.\n",
    "        remove_repetion: Removes repeated words from the text.\n",
    "        append_stopwords_list: Appends additional stopwords to the existing list.\n",
    "        remove_stopwords: Removes stopwords from the text.\n",
    "        remove_n: Removes words with length less than or equal to n from the text.\n",
    "        remove_numbers: Removes or filters out numbers from the text.\n",
    "        remove_gerund: Removes gerund endings from verbs in the text.\n",
    "        remove_infinitive: Removes infinitive endings from verbs in the text.\n",
    "        filter_by_idf: Filters out words based on their inverse document frequency.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noadverbs: bool = False, noadjectives: bool = False, noverbs: bool = False,\n",
    "                 noentities: bool = False, language: str = 'en', remove_list: bool = False,stopwords=[]):\n",
    "        \"\"\"Initialize the PreProcessing object.\n",
    "\n",
    "        Args:\n",
    "            noadverbs (bool, optional): Flag to indicate whether to remove adverbs. Defaults to False.\n",
    "            noadjectives (bool, optional): Flag to indicate whether to remove adjectives. Defaults to False.\n",
    "            noverbs (bool, optional): Flag to indicate whether to remove verbs. Defaults to False.\n",
    "            noentities (bool, optional): Flag to indicate whether to remove named entities. Defaults to False.\n",
    "            remove_list (bool, optional): Flag to indicate whether to remove stopwords. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.noadverbs = noadverbs\n",
    "        self.noadjectives = noadjectives\n",
    "        self.noverbs = noverbs\n",
    "        self.noentities = noentities\n",
    "        self.remove_list = remove_list\n",
    "        self.punctuation = (\n",
    "                r'\\(|!|\"|#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|\\.|\\/|'\n",
    "                r':|;|<|=|>|\\?|\\@|\\[|\\]|\\^|_|`|\\{|\\}|~|\\||'\n",
    "                r'\\r\\n|\\n|\\r|\\\\\\)'\n",
    "        )\n",
    "        # self.nlp = self._load_spacy_model(language)\n",
    "        # self.stopwords = [unidecode(x).lower() for x in list(self.nlp.Defaults.stop_words)]\n",
    "        self.stopwords=stopwords\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _process_text(self, text: Union[str, list], function: Callable) -> Union[str, list]:\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            return function(text)\n",
    "        elif isinstance(text, list):\n",
    "            return [function(x) for x in text]\n",
    "        return ''\n",
    "    \n",
    "    \n",
    "    def lowercase_unidecode(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Convert the given text to lowercase and remove any diacritical marks (accents).\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text to be processed. It can be either a string or a list of strings.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text. If the input is a string, the output will be a string. If the input is a list,\n",
    "            the output will be a list of strings.\n",
    "\n",
    "        Example:\n",
    "            >>> pre_processor = PreProcessor()\n",
    "            >>> text = \"Caf√©\"\n",
    "            >>> pre_processor.lowercase_unidecode(text)\n",
    "            'cafe'\n",
    "        \"\"\"\n",
    "        from unidecode import unidecode\n",
    "        text = self._process_text(text, lambda value: value.lower())\n",
    "        text = self._process_text(text, unidecode)\n",
    "        return text\n",
    "\n",
    "    def remove_urls(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes URLs from the given text or list of texts.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text or list of texts from which to remove URLs.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The text or list of texts with URLs removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'http\\S+ *', '', value).strip())\n",
    "\n",
    "    def remove_tweet_marking(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes tweet markings (e.g., @mentions and #hashtags) from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text or list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text or list of processed texts with tweet markings removed.\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'(@|#)\\S+ *', '', value).strip())\n",
    "\n",
    "    def remove_html_tags(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes HTML tags from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text or list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text or list of processed texts with HTML tags removed.\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'<.*?> *', '', value).strip())\n",
    "\n",
    "    def remove_punctuation(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes punctuation from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The text from which punctuation needs to be removed.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The text with punctuation removed.\n",
    "        \"\"\"\n",
    "        text = self._process_text(text, lambda value: re.sub(self.punctuation, ' ', value))\n",
    "        text = self._process_text(text, lambda value: re.sub(' {2,}', ' ', value).strip())\n",
    "        return text\n",
    "\n",
    "    def remove_repetition(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes repeated words in the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of words.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text with repeated words removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', value))\n",
    "\n",
    "    def append_stopwords_list(self, stopwords: list) -> None:\n",
    "        \"\"\"Appends additional stopwords to the existing list of stopwords.\n",
    "\n",
    "        Parameters:\n",
    "        stopwords (list): A list of stopwords to be appended.\n",
    "\n",
    "        \"\"\"\n",
    "        self.stopwords.extend(stopwords)\n",
    "\n",
    "    def remove_stopwords(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes stopwords from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text from which stopwords need to be removed.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text with stopwords removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(rf'\\b({\"|\".join(self.stopwords)})\\b *', '', value).strip())\n",
    "\n",
    "    \n",
    "\n",
    "    def remove_n(self, text: Union[str, list], n: int) -> Union[str, list]:\n",
    "        \"\"\"Removes words of length 1 to n followed by the word 'pri' from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of texts to process.\n",
    "            n (int): The maximum length of words to remove.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text or list of processed texts.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(rf'(\\b|^)\\w{{1,{n}}}(\\b|$) ?', '', value).strip())\n",
    "\n",
    "    def remove_numbers(self, text: Union[str, list], mode: str = 'replace') -> Union[str, list]:\n",
    "        \"\"\"Removes or replaces numbers in the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of texts.\n",
    "            mode (str, optional): The mode of operation. Defaults to 'replace'.\n",
    "                - 'filter': Removes the numbers from the text.\n",
    "                - 'replace': Replaces the numbers with an empty string.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text or list of processed texts.\n",
    "        \"\"\"\n",
    "        if mode == \"filter\":\n",
    "            return self._process_text(text, lambda value: '' if re.search('[0-9]', value) else value)\n",
    "        elif mode == \"replace\":\n",
    "            return self._process_text(text, lambda value: re.sub('[0-9] *', '', value))\n",
    "\n",
    "    def remove_gerund(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes the gerund form '-ndo' from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text with the gerund form removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'ndo\\b', '', value))\n",
    "\n",
    "    def remove_infinitive(self, text: Union[str, list]) -> Union[str, list]:\n",
    "        \"\"\"Removes the infinitive form of verbs from the given text.\n",
    "\n",
    "        Args:\n",
    "            text (Union[str, list]): The input text or list of texts to process.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, list]: The processed text with infinitive forms removed.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._process_text(text, lambda value: re.sub(r'r\\b', '', value))\n",
    "    \n",
    "    \n",
    "    def detect_language(self,text):\n",
    "        import langdetect\n",
    "        try:\n",
    "            d=langdetect.detect_langs(text)\n",
    "            # Trasforma la lista in un dizionario\n",
    "            langs_dict = {lang.lang: lang.prob for lang in d}\n",
    "            best_lang=max(langs_dict,key=langs_dict.get)\n",
    "            best_lang=best_lang if langs_dict[best_lang]>=0.7 else 'unk'\n",
    "            return best_lang    \n",
    "        except langdetect.LangDetectException as e:\n",
    "            return 'unk'\n",
    "        return None\n",
    "\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "# here the funziona to call to preprocess the text\n",
    "def preprocess_text(text,stopwords=stopwords):\n",
    "    \n",
    "    pp=PreProcessing(language='en',stopwords=stopwords)\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    text = pp.lowercase_unidecode(text)\n",
    "    \n",
    "    if pp.detect_language(text)!='en':\n",
    "        return \"\"\n",
    "    \n",
    "    text = pp.remove_stopwords(text)\n",
    "    text = pp.remove_tweet_marking(text)\n",
    "    text = pp.remove_urls(text)\n",
    "    text = pp.remove_repetition(text)\n",
    "    text = pp.remove_punctuation(text)\n",
    "    text = pp.remove_numbers(text)\n",
    "    text = pp.remove_n(text, n=3)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f9e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----An entry of ~/telegram_2024/usc-tg-24-us-election/extracted/channel_2144322933/2024-03.tsv.gz\n",
    "   id       user_id text   timestamp  bot_flag  via_bot_id  \\\n",
    "0   5  6.897729e+09  Hii  1711518674       NaN         NaN   \n",
    "\n",
    "   via_business_bot_id  reply_to_msg_id  fwd_flag  fwd_from_id media_type  \\\n",
    "0                  NaN              NaN       NaN          NaN        NaN   \n",
    "\n",
    "   views  forwards  replies  reactions reaction_json  \n",
    "0    NaN       NaN        0          0           NaN\n",
    "\n",
    "\n",
    "üìå Tabelle nel DB: [('chats',)]\n",
    "‚úÖ chats.db - Tabella 'chats'\n",
    "  type_and_id                            token parent     timestamp\n",
    "0        None           [keyword] thedemocrats   None  1.722583e+09\n",
    "1        None  [keyword] makeamericagreatagain   None  1.722583e+09\n",
    "2        None                   [keyword] MAGA   None  1.722583e+09\n",
    "3        None            [keyword] Nikki Haley   None  1.722583e+09\n",
    "4        None  [keyword] Robert F. Kennedy Jr.   None  1.722583e+09\n",
    "‚úÖ discovery_edges.csv.gz, \n",
    "Il timestamp da l'ultima volta che hanno visitato quel gruppo ma questo significa che non √® davvero indicativo di una timeline \n",
    "\n",
    "          type_and_id              parent     timestamp\n",
    "0  channel_1306559115  channel_1840578235  1.722586e+09\n",
    "1  channel_2036850729  channel_1840578235  1.722586e+09\n",
    "2  channel_1941222046  channel_1840578235  1.722586e+09\n",
    "3  channel_1749991917  channel_1840578235  1.722586e+09\n",
    "4  channel_1581117699  channel_1840578235  1.722586e+09\n",
    "‚úÖ first_nodes.csv.gz\n",
    "          type_and_id                    token                      parent\n",
    "0  channel_2036421633               trump2024e         [keyword] Trump2024\n",
    "1  channel_2178554925  biden_has_left_the_chat             [keyword] Biden\n",
    "2  channel_2095394414             speech_biden      [keyword] Joseph Biden\n",
    "3  channel_2202860593       republicanpartyeth  [keyword] Republican party\n",
    "4  channel_2157448164      republican_partysol  [keyword] Republican party\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(cpu_count())\n",
    "\n",
    "# Lista per accumulare tutti i messaggi in inglese\n",
    "all_english_messages = []\n",
    "\n",
    "# Wrapper function for multiprocessing\n",
    "def process_file(args):\n",
    "    file, channel_id, token = args\n",
    "    try:\n",
    "        df = pd.read_csv(file, sep='\\t', compression='gzip', usecols=['text','timestamp'])\n",
    "        df = df.dropna(subset=['text'])\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        \n",
    "        df['text_preprocessed'] = df['text'].apply(preprocess_text)\n",
    "        \n",
    "        df = df[df['text_preprocessed']!=\"\"]\n",
    "        \n",
    "        df['channel_id'] = channel_id\n",
    "        df['token'] = token\n",
    "        return df if not df.empty else None\n",
    "    except Exception as e:\n",
    "        print(f'Errore nel file {file}: {e}')\n",
    "        return None\n",
    "\n",
    "debug_count = 0\n",
    "debug_count_iterations = 0\n",
    "# Prepare list of files to process\n",
    "file_args = []\n",
    "for _, row in df_first_nodes.iterrows():\n",
    "    debug_count_iterations+=1\n",
    "    channel_id = row['type_and_id']\n",
    "    token = row['token']\n",
    "    channel_path = os.path.join(extracted_dir, channel_id)\n",
    "    if not os.path.isdir(channel_path):\n",
    "        debug_count+=1\n",
    "        continue\n",
    "    files = []\n",
    "    for month in [8, 9, 10, 11]: \n",
    "        month_str = f\"{month:02d}\"  \n",
    "        files.extend(glob(os.path.join(channel_path, f'2024-{month_str}.tsv.gz')))\n",
    "\n",
    "    file_args.extend([(file, channel_id, token) for file in files])\n",
    "    \n",
    "\"\"\"\n",
    "file_args: [\n",
    "  (\"extracted/channel_123456789/2024-08.tsv.gz\", \"channel_123456789\", \"trump2024\"),\n",
    "  (\"extracted/channel_123456789/2024-09.tsv.gz\", \"channel_123456789\", \"trump2024\")\n",
    "]\n",
    "\"\"\"\n",
    "    \n",
    "print(\"len(df_first_nodes) 284\")\n",
    "print(\"print(df_first_nodes.type_and_id.nunique()) 247\")\n",
    "print(\"numero di NaN\", str(sum(pd.isna(entry[1]) for entry in file_args)))\n",
    "print(\"numero di messaggi in file_args\"+str(len(file_args)))\n",
    "print(\"numero di channel_id distinte in file_args[1](channel_id) \"+str(len({entry[1] for entry in file_args})))\n",
    "print(\"debug count: \" + str(debug_count))\n",
    "print(\"debug cunt iterations\" + str(debug_count_iterations))\n",
    "print(\"in seguito verra stampato df_english_preprocessed_messages.channel_id.nunique() 94\")\n",
    "\n",
    "# Use multiprocessing to process files\n",
    "results=[]\n",
    "with Pool(cpu_count()) as pool:\n",
    "    pbar=tqdm(total=len(file_args))\n",
    "    for res in pool.imap_unordered(process_file, file_args):\n",
    "        pbar.update(1)\n",
    "        results.append(res)\n",
    "\n",
    "# Filter out None results and concatenate\n",
    "all_english_messages = [df for df in results if df is not None]\n",
    "print(\"--1--\")\n",
    "df_english_preprocessed_messages = pd.concat(all_english_messages, ignore_index=True)\n",
    "print(\"--2--df_english_preprocessed_messages prima di drop_duplicates \", str(df_english_preprocessed_messages.channel_id.nunique()))\n",
    "df_english_preprocessed_messages=df_english_preprocessed_messages.drop_duplicates(subset=['text_preprocessed'])\n",
    "print(\"--3--\")\n",
    "df_english_preprocessed_messages.to_csv(\"../material/english_preprocessed_messages.tsv.gz\", sep='\\t', index=False, compression='gzip')\n",
    "print(\"--4--\")\n",
    "print(df_english_preprocessed_messages.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98fd5d1-2d40-4a7b-aab5-37a3c11c5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1Ô∏è‚É£ Carica i messaggi pre-processati\n",
    "df_english_preprocessed_messages = pd.read_csv(\n",
    "    \"../material/english_preprocessed_messages.tsv.gz\",\n",
    "    sep='\\t',\n",
    "    compression='gzip'\n",
    ")\n",
    "\n",
    "# 2Ô∏è‚É£ Converte il timestamp (in secondi) in data leggibile\n",
    "df_english_preprocessed_messages['date'] = pd.to_datetime(\n",
    "    df_english_preprocessed_messages['timestamp'],\n",
    "    unit='s'\n",
    ")\n",
    "\n",
    "# 3Ô∏è‚É£ Istogramma di tutte le date dei messaggi\n",
    "df_english_preprocessed_messages['date'].hist(bins=120)\n",
    "plt.title(\"üìÖ Tutti i messaggi pre-processati\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Numero di messaggi\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4Ô∏è‚É£ Numero di testi pre-processati\n",
    "print(\"Numero di testi pre-processati:\", len(df_english_preprocessed_messages['text_preprocessed']))\n",
    "\n",
    "# 5Ô∏è‚É£ Numero di canali unici\n",
    "print(\"---\")\n",
    "print(\"Numero di canali unici:\", df_english_preprocessed_messages['channel_id'].nunique())\n",
    "print(\"---\")\n",
    "\n",
    "# 6Ô∏è‚É£ Istogramma della data dell‚Äôultimo messaggio per ciascun canale\n",
    "df_english_preprocessed_messages.sort_values(by='date') \\\n",
    "    .drop_duplicates(subset='channel_id', keep='last')['date'] \\\n",
    "    .hist(bins=100)\n",
    "plt.title(\"üìÖ Ultima data di messaggio per ciascun canale\")\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Numero di canali\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd8049-f9ef-4edd-97f4-7ee210cca814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "model_name = 'all-distilroberta-v1'\n",
    "embedding_model=SentenceTransformer(model_name)\n",
    "\n",
    "umap_params = {'n_components': 5, 'n_neighbors': 5, 'min_dist': 0.0}\n",
    "\n",
    "hdbscan_params = {'min_cluster_size': 500,'min_samples':100,'prediction_data':True}\n",
    "\n",
    "if os.path.exists(f'final/final_embeddings_{model_name}.npy'):\n",
    "    embeddings = np.load(f'final/final_embeddings_{model_name}.npy')\n",
    "    print(f'embedding {model_name} loaded')\n",
    "else:\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    embeddings = embedding_model.encode(df_english_preprocessed_messages['text_preprocessed'].tolist(), show_progress_bar=True, device=device)\n",
    "    np.save(f'final/final_embeddings_{model_name}.npy', embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68119eb-a332-4d9a-8183-5cfe72bf9451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342381c-933b-46de-9181-fbd3e95d4b0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "umap_model = UMAP(**umap_params)\n",
    "hdbscan_model = HDBSCAN(**hdbscan_params)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "max_features_vectorizer = 1024\n",
    "min_df_vectorizer = 0.01\n",
    "max_df_vectorizer = 0.99 \n",
    "\n",
    "#min_df = 0.01 ‚Üí una parola deve apparire in almeno 100 documenti per essere tenuta.\n",
    "#max_df = 0.99 ‚Üí se una parola appare in pi√π di 9.900 documenti, viene scartata (tipo \"the\", \"and\", ecc.).\n",
    "#max_features = 1024 ‚Üí anche se hai 10.000 parole valide, tiene solo le 1024 pi√π frequenti.\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "  max_features=max_features_vectorizer, \n",
    "  min_df=min_df_vectorizer,\n",
    "  max_df=max_df_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b52ac-512b-4e4c-86ca-789cfa3bba1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=None,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    verbose=True,\n",
    "    top_n_words=20,\n",
    "    language = 'english', \n",
    "    vectorizer_model=vectorizer_model\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(df_english_preprocessed_messages['text_preprocessed'],embeddings=embeddings)\n",
    "print(f\"Execution time for {model_name} UMAP: {time.time()-t0}s\")\n",
    "\n",
    "topics=np.array(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82084cc5-85ea-4562-8908-caba625b95fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.save(f'final/final_{model_name}.pkl')\n",
    "topics=np.array(topic_model.topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a7507-ba78-4825-b2b0-e1eaac0dd449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model=BERTopic.load(f'final/final_{model_name}.pkl')\n",
    "topics=np.array(topic_model.topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08c2cd-c974-430c-9b05-010b5a938544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=-1,n_words=20, width = 350,height=450)\n",
    "\n",
    "#top_n_topics=-1\tMostra tutti i topic (eccetto outlier -1)\ttop_n_topics=5 mostrerebbe solo i primi 5 topic pi√π grandi\n",
    "#n_words=20\tMostra 20 parole per ogni topic nel grafico\tSe vuoi vederne solo 10, metti n_words=10\n",
    "#width=350\tLarghezza (in pixel) del grafico\tCambia la dimensione orizzontale del plot\n",
    "#height=450\tAltezza (in pixel) del grafico\tCambia la dimensione verticale del plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832737dc-03ef-4603-9060-3f9fe1c8e1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = df_english_preprocessed_messages['text_preprocessed']\n",
    "timestamps = pd.to_datetime(df_english_preprocessed_messages['date'],format=\"%Y-%m-%d\")\n",
    "\n",
    "topics_over_time = topic_model.topics_over_time(\n",
    "    list(df_english_preprocessed_messages['text_preprocessed']),\n",
    "    list(df_english_preprocessed_messages['date']),\n",
    "    nr_bins = timestamps.nunique(),\n",
    "    global_tuning = False,\n",
    "    evolution_tuning = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a204319-726a-4322-a17a-f7a0914cd573",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics_over_time(\n",
    "    topics_over_time,\n",
    "    top_n_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e7d7d4-1b49-4e28-aea5-0560db58d86a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umap_model=topic_model.umap_model\n",
    "reduced_embeddings=umap_model.transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f9fcb-ee0f-4dfa-9c80-aed2d92ed0a1",
   "metadata": {},
   "source": [
    "- reduce outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3be546-0e58-42a4-906e-bb80a8b0d88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_topics = topic_model.reduce_outliers(list(texts), topics , strategy=\"c-tf-idf\", threshold=0.1)\n",
    "\n",
    "#Quando BERTopic prova a riassegnare un outlier, confronta il suo contenuto testuale con i topic usando una misura di similarit√† tra vettori TF-IDF.\n",
    "#Di solito si tratta di una similarit√† coseno, che varia tra:\n",
    "#0.0 = nessuna somiglianza\n",
    "#1.0 = perfetta somiglianza\n",
    "#0.1 √® molto poco\n",
    "\n",
    "\n",
    "\n",
    "topic_model.update_topics(list(texts), topics=new_topics,\n",
    "                          vectorizer_model=vectorizer_model,top_n_words=20)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1024c9-7f2a-44a6-9517-31d0c220b8ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_topics=np.array(new_topics)\n",
    "pd.Series(new_topics).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adcd337-a7ac-4f44-9572-5055aee64b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5122a-76a1-4dac-8589-7788e4f11657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-bertopic_env]",
   "language": "python",
   "name": "conda-env-.conda-bertopic_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
